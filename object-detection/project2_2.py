# -*- coding: utf-8 -*-
"""Project2-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19aK5l6_m0vGjRGmbo0FFx1W2xRhfzOEO
"""

# Import libraries
import locale
locale.getpreferredencoding = lambda: "UTF-8"
!pip install ultralytics kagglehub --quiet
import os
import random
import glob
import cv2
import time
import matplotlib.pyplot as plt
from ultralytics import YOLO
from PIL import Image
import kagglehub
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np

# Download dataset
data_dir = kagglehub.dataset_download("pkdarabi/cardetection")

# YOLOv8 모델 초기화
print("⏳Loading YOLOv8 model...")
model = YOLO("yolov8n.pt")

image_dir = os.path.join(data_dir, 'car/train/images')
image_files = os.listdir(image_dir)

num_samples = 9
rand_images = random.sample(image_files, num_samples)

print("Displaying sample images from the dataset...")
fig, axes = plt.subplots(3, 3, figsize=(11, 11))
for i, ax in enumerate(axes.flatten()):
    image = rand_images[i]
    ax.imshow(cv2.cvtColor(cv2.imread(os.path.join(image_dir, image)), cv2.COLOR_BGR2RGB))
    ax.set_title(f"Image {i+1}")
    ax.axis("off")
plt.tight_layout()
plt.show()

def normalize_image(image):
    return image / 255.0

# Data augmentation
print("⏳Applying data augmentation to sample images...")
augmentation_pipeline = A.Compose([
    A.HorizontalFlip(p=0.5),  # 좌우 뒤집기
    A.Rotate(limit=15, p=0.5),  # 랜덤 회전
    A.RandomScale(scale_limit=0.2, p=0.5),  # 랜덤 크기 조정
    ToTensorV2()  # 텐서 변환
])

aug_image_path = os.path.join(image_dir, random.choice(image_files))
aug_image = cv2.imread(aug_image_path)
aug_image = cv2.cvtColor(aug_image, cv2.COLOR_BGR2RGB)
aug_image = normalize_image(aug_image)
aug_result = augmentation_pipeline(image=aug_image)
aug_image_augmented = aug_result['image']

plt.imshow(aug_image_augmented.permute(1, 2, 0))  # (C, H, W) → (H, W, C)
plt.title("Augmented Sample Image")
plt.axis("off")
plt.show()

# Train the model with transfer learning
train_data = os.path.join(data_dir, "car/data.yaml")

print("Starting YOLOv8 training with transfer learning...🏃")
model.train(
    data=train_data,  # 데이터셋 설정 파일
    epochs=30,  # 학습 반복 수
    imgsz=640,  # 이미지 크기
    batch=16,  # 배치 크기
    optimizer="Adam",  # 옵티마이저
    augment=True  # 데이터 증강 활성화
)

# Evaluate the model
print("Evaluating the trained model...🖋️")
metrics = model.val()

# IoU 출력
iou_thresholds = metrics.results_dict.get("metrics/iou_thres", [])
mean_iou = np.mean(iou_thresholds) if iou_thresholds else 0
print(f"Mean IoU: {mean_iou:.4f}")

# 기존 지표 출력
print("Model Evaluation Metrics:")
print(f"Precision: {metrics.results_dict['metrics/precision(B)']}")
print(f"Recall: {metrics.results_dict['metrics/recall(B)']}")
print(f"mAP@50: {metrics.results_dict['metrics/mAP50(B)']}")
print(f"mAP@50-95: {metrics.results_dict['metrics/mAP50-95(B)']}")

# Process a sample validation set for predictions
valid_images_path = os.path.join(data_dir, "car/test/images")
valid_image_files = [f for f in os.listdir(valid_images_path) if f.endswith(".jpg")]

if len(valid_image_files) > 0:
    print("Visualizing predictions on the validation set...")
    fig, axes = plt.subplots(3, 3, figsize=(11, 11))
    step_size = max(1, len(valid_image_files) // 9)
    selected_images = [valid_image_files[i] for i in range(0, len(valid_image_files), step_size)]

    for i, ax in enumerate(axes.flatten()):
        if i < len(selected_images):
            image_path = os.path.join(valid_images_path, selected_images[i])
            image = cv2.imread(image_path)
            if image is not None:
                results = model.predict(source=image, imgsz=640, conf=0.5)
                annotated_image = results[0].plot(line_width=1)
                ax.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))
            else:
                print(f"Failed to load image: {image_path}")
        ax.axis("off")
    plt.tight_layout()
    plt.show()

# Input 및 Output 설정
input_video_path = f"{data_dir}/video.mp4"  # 입력 비디오 경로
output_video_path = "result_out.mp4"  # 변환된 비디오 경로

print("Running object detection on video...")
results = model.predict(source=input_video_path, save=True, save_txt=True, save_conf=True)

# 저장된 결과 디렉토리 추출
if results and hasattr(results[0], 'save_dir'):
    result_dir = results[0].save_dir
    print(f"Results saved to: {result_dir}")
else:
    raise AttributeError("Could not retrieve save directory from results.")

# AVI 파일 경로
avi_path = os.path.join(result_dir, "video.avi")

if os.path.exists(avi_path):
    print(f"AVI file found at: {avi_path}")

    # AVI -> MP4 변환
    !ffmpeg -y -loglevel panic -i {avi_path} {output_video_path}
    print(f"MP4 file saved at: {output_video_path}")

    # 탐지된 이미지 저장 경로 설정
    detect_image_dir = os.path.join(result_dir, "frames")
    os.makedirs(detect_image_dir, exist_ok=True)

    print("Extracting frames from AVI file...")
    cap = cv2.VideoCapture(avi_path)
    frame_count = 0
    save_interval = 10  # 10 프레임마다 이미지 저장

    # AVI 파일에서 프레임 저장
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_count % save_interval == 0:  # 간격에 따라 저장
            frame_path = os.path.join(detect_image_dir, f"frame_{frame_count:04d}.jpg")
            cv2.imwrite(frame_path, frame)
        frame_count += 1
    cap.release()
    print(f"Frames extracted and saved to: {detect_image_dir}")

    # 저장된 이미지 중 랜덤하게 9개 선택
    image_files = sorted(glob.glob(os.path.join(detect_image_dir, '*.jpg')))
    if image_files:
        random_images = random.sample(image_files, min(9, len(image_files)))
        print("Displaying 9 random detection images...")
        fig, axes = plt.subplots(3, 3, figsize=(15, 15))
        axes = axes.flatten()
        for img_path, ax in zip(random_images, axes):
            img = Image.open(img_path)
            ax.imshow(img)
            ax.axis('off')
            ax.set_title(os.path.basename(img_path))
        plt.tight_layout()
        plt.show()
    else:
        print("No detection images found to display.")

else:
    raise FileNotFoundError(f"AVI file not found at: {avi_path}")

# Export the trained model
print("Exporting the model in ONNX format...")
model.export(format="onnx")
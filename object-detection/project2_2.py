# -*- coding: utf-8 -*-
"""Project2-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19aK5l6_m0vGjRGmbo0FFx1W2xRhfzOEO
"""

# Import libraries
import locale
locale.getpreferredencoding = lambda: "UTF-8"
!pip install ultralytics kagglehub --quiet
import os
import random
import glob
import cv2
import time
import matplotlib.pyplot as plt
from ultralytics import YOLO
from PIL import Image
import kagglehub
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np

# Download dataset
data_dir = kagglehub.dataset_download("pkdarabi/cardetection")

# YOLOv8 ëª¨ë¸ ì´ˆê¸°í™”
print("â³Loading YOLOv8 model...")
model = YOLO("yolov8n.pt")

image_dir = os.path.join(data_dir, 'car/train/images')
image_files = os.listdir(image_dir)

num_samples = 9
rand_images = random.sample(image_files, num_samples)

print("Displaying sample images from the dataset...")
fig, axes = plt.subplots(3, 3, figsize=(11, 11))
for i, ax in enumerate(axes.flatten()):
    image = rand_images[i]
    ax.imshow(cv2.cvtColor(cv2.imread(os.path.join(image_dir, image)), cv2.COLOR_BGR2RGB))
    ax.set_title(f"Image {i+1}")
    ax.axis("off")
plt.tight_layout()
plt.show()

def normalize_image(image):
    return image / 255.0

# Data augmentation
print("â³Applying data augmentation to sample images...")
augmentation_pipeline = A.Compose([
    A.HorizontalFlip(p=0.5),  # ì¢Œìš° ë’¤ì§‘ê¸°
    A.Rotate(limit=15, p=0.5),  # ëœë¤ íšŒì „
    A.RandomScale(scale_limit=0.2, p=0.5),  # ëœë¤ í¬ê¸° ì¡°ì •
    ToTensorV2()  # í…ì„œ ë³€í™˜
])

aug_image_path = os.path.join(image_dir, random.choice(image_files))
aug_image = cv2.imread(aug_image_path)
aug_image = cv2.cvtColor(aug_image, cv2.COLOR_BGR2RGB)
aug_image = normalize_image(aug_image)
aug_result = augmentation_pipeline(image=aug_image)
aug_image_augmented = aug_result['image']

plt.imshow(aug_image_augmented.permute(1, 2, 0))  # (C, H, W) â†’ (H, W, C)
plt.title("Augmented Sample Image")
plt.axis("off")
plt.show()

# Train the model with transfer learning
train_data = os.path.join(data_dir, "car/data.yaml")

print("Starting YOLOv8 training with transfer learning...ğŸƒ")
model.train(
    data=train_data,  # ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼
    epochs=30,  # í•™ìŠµ ë°˜ë³µ ìˆ˜
    imgsz=640,  # ì´ë¯¸ì§€ í¬ê¸°
    batch=16,  # ë°°ì¹˜ í¬ê¸°
    optimizer="Adam",  # ì˜µí‹°ë§ˆì´ì €
    augment=True  # ë°ì´í„° ì¦ê°• í™œì„±í™”
)

# Evaluate the model
print("Evaluating the trained model...ğŸ–‹ï¸")
metrics = model.val()

# IoU ì¶œë ¥
iou_thresholds = metrics.results_dict.get("metrics/iou_thres", [])
mean_iou = np.mean(iou_thresholds) if iou_thresholds else 0
print(f"Mean IoU: {mean_iou:.4f}")

# ê¸°ì¡´ ì§€í‘œ ì¶œë ¥
print("Model Evaluation Metrics:")
print(f"Precision: {metrics.results_dict['metrics/precision(B)']}")
print(f"Recall: {metrics.results_dict['metrics/recall(B)']}")
print(f"mAP@50: {metrics.results_dict['metrics/mAP50(B)']}")
print(f"mAP@50-95: {metrics.results_dict['metrics/mAP50-95(B)']}")

# Process a sample validation set for predictions
valid_images_path = os.path.join(data_dir, "car/test/images")
valid_image_files = [f for f in os.listdir(valid_images_path) if f.endswith(".jpg")]

if len(valid_image_files) > 0:
    print("Visualizing predictions on the validation set...")
    fig, axes = plt.subplots(3, 3, figsize=(11, 11))
    step_size = max(1, len(valid_image_files) // 9)
    selected_images = [valid_image_files[i] for i in range(0, len(valid_image_files), step_size)]

    for i, ax in enumerate(axes.flatten()):
        if i < len(selected_images):
            image_path = os.path.join(valid_images_path, selected_images[i])
            image = cv2.imread(image_path)
            if image is not None:
                results = model.predict(source=image, imgsz=640, conf=0.5)
                annotated_image = results[0].plot(line_width=1)
                ax.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))
            else:
                print(f"Failed to load image: {image_path}")
        ax.axis("off")
    plt.tight_layout()
    plt.show()

# Input ë° Output ì„¤ì •
input_video_path = f"{data_dir}/video.mp4"  # ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
output_video_path = "result_out.mp4"  # ë³€í™˜ëœ ë¹„ë””ì˜¤ ê²½ë¡œ

print("Running object detection on video...")
results = model.predict(source=input_video_path, save=True, save_txt=True, save_conf=True)

# ì €ì¥ëœ ê²°ê³¼ ë””ë ‰í† ë¦¬ ì¶”ì¶œ
if results and hasattr(results[0], 'save_dir'):
    result_dir = results[0].save_dir
    print(f"Results saved to: {result_dir}")
else:
    raise AttributeError("Could not retrieve save directory from results.")

# AVI íŒŒì¼ ê²½ë¡œ
avi_path = os.path.join(result_dir, "video.avi")

if os.path.exists(avi_path):
    print(f"AVI file found at: {avi_path}")

    # AVI -> MP4 ë³€í™˜
    !ffmpeg -y -loglevel panic -i {avi_path} {output_video_path}
    print(f"MP4 file saved at: {output_video_path}")

    # íƒì§€ëœ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ ì„¤ì •
    detect_image_dir = os.path.join(result_dir, "frames")
    os.makedirs(detect_image_dir, exist_ok=True)

    print("Extracting frames from AVI file...")
    cap = cv2.VideoCapture(avi_path)
    frame_count = 0
    save_interval = 10  # 10 í”„ë ˆì„ë§ˆë‹¤ ì´ë¯¸ì§€ ì €ì¥

    # AVI íŒŒì¼ì—ì„œ í”„ë ˆì„ ì €ì¥
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_count % save_interval == 0:  # ê°„ê²©ì— ë”°ë¼ ì €ì¥
            frame_path = os.path.join(detect_image_dir, f"frame_{frame_count:04d}.jpg")
            cv2.imwrite(frame_path, frame)
        frame_count += 1
    cap.release()
    print(f"Frames extracted and saved to: {detect_image_dir}")

    # ì €ì¥ëœ ì´ë¯¸ì§€ ì¤‘ ëœë¤í•˜ê²Œ 9ê°œ ì„ íƒ
    image_files = sorted(glob.glob(os.path.join(detect_image_dir, '*.jpg')))
    if image_files:
        random_images = random.sample(image_files, min(9, len(image_files)))
        print("Displaying 9 random detection images...")
        fig, axes = plt.subplots(3, 3, figsize=(15, 15))
        axes = axes.flatten()
        for img_path, ax in zip(random_images, axes):
            img = Image.open(img_path)
            ax.imshow(img)
            ax.axis('off')
            ax.set_title(os.path.basename(img_path))
        plt.tight_layout()
        plt.show()
    else:
        print("No detection images found to display.")

else:
    raise FileNotFoundError(f"AVI file not found at: {avi_path}")

# Export the trained model
print("Exporting the model in ONNX format...")
model.export(format="onnx")